{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We're using here a ``synchronous`` memmory saver, which means that untill each llm/tool call is not done, we won't get any output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add persistence to the agent\n",
    "#by using a checkpointer\n",
    "#\"checkpoints\" the state of the agent after and between every node\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "# stack = AsyncExitStack()\n",
    "\n",
    "# memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "#it didn't work, so i used a sync solution answer\n",
    "#https://community.deeplearning.ai/t/lesson-4-persistence-and-streaming-attributeerror-generatorcontextmanager-object-has-no-attribute-get-next-version/697391/3\n",
    "\n",
    "from contextlib import ExitStack\n",
    "\n",
    "stack = ExitStack()\n",
    "memory = stack.enter_context(SqliteSaver.from_conn_string(\":memory:\"))\n",
    "\n",
    "#it will use a built-in database of sqlite for this\n",
    "#so when refreshing the notebook it'll dissapear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "        #we pass the checkpointer param here when compiling() the agent graph\n",
    "        \n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding streaming concept to the agent\n",
    "\n",
    "#1. We might care of streaming the individual messages\n",
    "#so this would be keeping track of the ai message that determines which action to take\n",
    "#and the observation message (result of taking that action/tool)\n",
    "\n",
    "#we can also stream the tokens of each llm output\n",
    "\n",
    "\n",
    "#we'll start streaming only the messages (not the tokens)\n",
    "messages = [HumanMessage(content=\"What is the weather in sf?\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we're now use a thread config\n",
    "\n",
    "#this'll be used to keep track of different threads\n",
    "#inside the persistence checkpointer we defined before\n",
    "\n",
    "#this can be REALLY usefull to allow us run multiple conversations at the same time\n",
    "#(e.g) application with many users\n",
    "thread = {\"configurable\": {\"thread_id\":\"1\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of calling `invoke()` to run our model, we just stream over the graph, with the selected `thread_id=1`we selected before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this way, **we'll be getting all the intermediate steps and messages** going on in our agent and tools untill it deliveries the end answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_2qQv5jXM7klQCJZFEPEIBqDA', 'function': {'arguments': '{\"query\":\"San Francisco weather today\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 151, 'total_tokens': 172, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-3ec895b4-03ea-4e25-a226-f6c753ad1314-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'San Francisco weather today'}, 'id': 'call_2qQv5jXM7klQCJZFEPEIBqDA', 'type': 'tool_call'}], usage_metadata={'input_tokens': 151, 'output_tokens': 21, 'total_tokens': 172, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'San Francisco weather today'}, 'id': 'call_2qQv5jXM7klQCJZFEPEIBqDA', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "[ToolMessage(content='[{\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1730464087, \\'localtime\\': \\'2024-11-01 05:28\\'}, \\'current\\': {\\'last_updated_epoch\\': 1730463300, \\'last_updated\\': \\'2024-11-01 05:15\\', \\'temp_c\\': 13.4, \\'temp_f\\': 56.0, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Patchy rain nearby\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/176.png\\', \\'code\\': 1063}, \\'wind_mph\\': 9.8, \\'wind_kph\\': 15.8, \\'wind_degree\\': 289, \\'wind_dir\\': \\'WNW\\', \\'pressure_mb\\': 1015.0, \\'pressure_in\\': 29.96, \\'precip_mm\\': 0.04, \\'precip_in\\': 0.0, \\'humidity\\': 82, \\'cloud\\': 52, \\'feelslike_c\\': 12.1, \\'feelslike_f\\': 53.7, \\'windchill_c\\': 12.1, \\'windchill_f\\': 53.7, \\'heatindex_c\\': 13.4, \\'heatindex_f\\': 56.0, \\'dewpoint_c\\': 10.5, \\'dewpoint_f\\': 51.0, \\'vis_km\\': 10.0, \\'vis_miles\\': 6.0, \\'uv\\': 1.0, \\'gust_mph\\': 15.4, \\'gust_kph\\': 24.7}}\"}, {\\'url\\': \\'https://world-weather.info/forecast/usa/san_francisco/january-2024/\\', \\'content\\': \\'Extended weather forecast in San Francisco. Hourly Week 10-Day 14-Day 30-Day Year. Detailed ‚ö° San Francisco Weather Forecast for January 2024 - day/night üå°Ô∏è temperatures, precipitations - World-Weather.info.\\'}]', name='tavily_search_results_json', tool_call_id='call_2qQv5jXM7klQCJZFEPEIBqDA')]\n",
      "[AIMessage(content='The current weather in San Francisco is 13.4¬∞C (56.0¬∞F) with patchy rain nearby. The wind is blowing from the west-northwest at 9.8 mph (15.8 kph), and the humidity is at 82%. The visibility is 10 kilometers (6 miles), and the UV index is low at 1.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 669, 'total_tokens': 745, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'stop', 'logprobs': None}, id='run-d2e73a2b-f368-4e47-b7d3-06cbd947ea77-0', usage_metadata={'input_tokens': 669, 'output_tokens': 76, 'total_tokens': 745, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]\n"
     ]
    }
   ],
   "source": [
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content='Could you please clarify your question? Are you asking about the current weather, a specific past date, or a particular location?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 151, 'total_tokens': 177, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'stop', 'logprobs': None}, id='run-dd170041-abd8-4c7e-80a5-fed9c1c8cd6a-0', usage_metadata={'input_tokens': 151, 'output_tokens': 26, 'total_tokens': 177, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Which was the weather, sorry?\")]\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "#if we used thread_id=2, it wouldn't work cuz it wouldn't have context from the past\n",
    "#the memmory saver would be inside another separate thread\n",
    "\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here, we are following up with the same conversation (`thread=1`, so it'll renember the content because the `memmory saver of the agent` is getting saved in the same `thread=1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_U0aSHjTBBVPNkXOID9YgXYAw', 'function': {'arguments': '{\"query\":\"Los Angeles weather today\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 935, 'total_tokens': 956, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-1af97bfb-140f-48cb-8bcc-9a10fbb8b957-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'Los Angeles weather today'}, 'id': 'call_U0aSHjTBBVPNkXOID9YgXYAw', 'type': 'tool_call'}], usage_metadata={'input_tokens': 935, 'output_tokens': 21, 'total_tokens': 956, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'Los Angeles weather today'}, 'id': 'call_U0aSHjTBBVPNkXOID9YgXYAw', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "{'messages': [ToolMessage(content='[{\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'Los Angeles\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 34.0522, \\'lon\\': -118.2428, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1730464362, \\'localtime\\': \\'2024-11-01 05:32\\'}, \\'current\\': {\\'last_updated_epoch\\': 1730464200, \\'last_updated\\': \\'2024-11-01 05:30\\', \\'temp_c\\': 15.1, \\'temp_f\\': 59.3, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Clear\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/113.png\\', \\'code\\': 1000}, \\'wind_mph\\': 4.9, \\'wind_kph\\': 7.9, \\'wind_degree\\': 112, \\'wind_dir\\': \\'ESE\\', \\'pressure_mb\\': 1014.0, \\'pressure_in\\': 29.93, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 71, \\'cloud\\': 4, \\'feelslike_c\\': 15.2, \\'feelslike_f\\': 59.4, \\'windchill_c\\': 15.2, \\'windchill_f\\': 59.4, \\'heatindex_c\\': 15.1, \\'heatindex_f\\': 59.3, \\'dewpoint_c\\': 10.0, \\'dewpoint_f\\': 49.9, \\'vis_km\\': 10.0, \\'vis_miles\\': 6.0, \\'uv\\': 1.0, \\'gust_mph\\': 6.9, \\'gust_kph\\': 11.2}}\"}, {\\'url\\': \\'https://world-weather.info/forecast/usa/los_angeles/january-2024/\\', \\'content\\': \\'Detailed ‚ö° Los Angeles Weather Forecast for January 2024 - day/night üå°Ô∏è temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Widgets ¬∞F. World; United States; California; Weather in Los Angeles; ... 11 +63¬∞ +52¬∞ 12 +61¬∞ +46¬∞ 13\\'}]', name='tavily_search_results_json', tool_call_id='call_U0aSHjTBBVPNkXOID9YgXYAw')]}\n",
      "{'messages': [AIMessage(content='The current weather in Los Angeles is 15.1¬∞C (59.3¬∞F) and clear. The wind is coming from the east-southeast at 4.9 mph (7.9 kph), with a humidity level of 71%. The visibility is 10 kilometers (6 miles), and the UV index is low at 1.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 1476, 'total_tokens': 1549, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'stop', 'logprobs': None}, id='run-8615c5d4-556a-4d53-a915-2289ee9f8a0c-0', usage_metadata={'input_tokens': 1476, 'output_tokens': 73, 'total_tokens': 1549, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What about LA?\")]\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "#if we used thread_id=2, it wouldn't work cuz it wouldn't have context from the past\n",
    "#the memmory saver would be inside another separate thread\n",
    "\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_zXFRJ8OhAyqpoONSIChdfvBs', 'function': {'arguments': '{\"query\":\"Spain weather today\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 2204, 'total_tokens': 2224, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 2048}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-a814d5e2-10f1-4001-a78a-512502660578-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'Spain weather today'}, 'id': 'call_zXFRJ8OhAyqpoONSIChdfvBs', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2204, 'output_tokens': 20, 'total_tokens': 2224, 'input_token_details': {'cache_read': 2048}, 'output_token_details': {'reasoning': 0}})]}\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'Spain weather today'}, 'id': 'call_zXFRJ8OhAyqpoONSIChdfvBs', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "{'messages': [ToolMessage(content='[{\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'Madrid\\', \\'region\\': \\'Madrid\\', \\'country\\': \\'Spain\\', \\'lat\\': 40.4, \\'lon\\': -3.683, \\'tz_id\\': \\'Europe/Madrid\\', \\'localtime_epoch\\': 1730464541, \\'localtime\\': \\'2024-11-01 13:35\\'}, \\'current\\': {\\'last_updated_epoch\\': 1730464200, \\'last_updated\\': \\'2024-11-01 13:30\\', \\'temp_c\\': 17.3, \\'temp_f\\': 63.1, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Partly cloudy\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/116.png\\', \\'code\\': 1003}, \\'wind_mph\\': 4.9, \\'wind_kph\\': 7.9, \\'wind_degree\\': 74, \\'wind_dir\\': \\'ENE\\', \\'pressure_mb\\': 1022.0, \\'pressure_in\\': 30.18, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 72, \\'cloud\\': 75, \\'feelslike_c\\': 17.3, \\'feelslike_f\\': 63.1, \\'windchill_c\\': 18.2, \\'windchill_f\\': 64.8, \\'heatindex_c\\': 18.2, \\'heatindex_f\\': 64.8, \\'dewpoint_c\\': 9.5, \\'dewpoint_f\\': 49.2, \\'vis_km\\': 10.0, \\'vis_miles\\': 6.0, \\'uv\\': 5.0, \\'gust_mph\\': 5.7, \\'gust_kph\\': 9.1}}\"}, {\\'url\\': \\'https://www.abc.net.au/news/2024-11-01/spain-valencia-flash-flood-158-killed-revised-deathtoll/104547162\\', \\'content\\': \"The Spanish prime minister is urging residents to stay home as the country\\'s weather bureau issues fresh weather alerts. ... 10h ago 10 hours ago Thu 31 Oct 2024 at ... Spain\\'s weather service has\"}]', name='tavily_search_results_json', tool_call_id='call_zXFRJ8OhAyqpoONSIChdfvBs')]}\n",
      "{'messages': [AIMessage(content=\"The current weather in San Francisco is 13.4¬∞C (56.0¬∞F) with patchy rain nearby, while in Madrid, Spain, it's 17.3¬∞C (63.1¬∞F) and partly cloudy. San Francisco is experiencing cooler and wetter conditions compared to Madrid, which is warmer and drier with partly cloudy skies.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 2725, 'total_tokens': 2795, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 2176}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'stop', 'logprobs': None}, id='run-98b62dd5-c578-4738-9499-67b0a01686c8-0', usage_metadata={'input_tokens': 2725, 'output_tokens': 70, 'total_tokens': 2795, 'input_token_details': {'cache_read': 2176}, 'output_token_details': {'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Compare this weather in sf with the one in Spain\")]\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "#if we used thread_id=2, it wouldn't work cuz it wouldn't have context from the past\n",
    "#the memmory saver would be inside another separate thread\n",
    "\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are in the same conversation memmory saved inside `thread=1`, we can ask it to compare the weathers from before with the one in Brazil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_fmuYhr86x3iCLReoPRJzSmEc', 'function': {'arguments': '{\"query\":\"Brazil weather today\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 2811, 'total_tokens': 2831, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 2688}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-9ec34feb-4f79-46a9-b7de-612799b252fc-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'Brazil weather today'}, 'id': 'call_fmuYhr86x3iCLReoPRJzSmEc', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2811, 'output_tokens': 20, 'total_tokens': 2831, 'input_token_details': {'cache_read': 2688}, 'output_token_details': {'reasoning': 0}})]}\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'Brazil weather today'}, 'id': 'call_fmuYhr86x3iCLReoPRJzSmEc', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "{'messages': [ToolMessage(content='[{\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'Brasilia\\', \\'region\\': \\'Distrito Federal\\', \\'country\\': \\'Brazil\\', \\'lat\\': -15.783, \\'lon\\': -47.917, \\'tz_id\\': \\'America/Sao_Paulo\\', \\'localtime_epoch\\': 1730464631, \\'localtime\\': \\'2024-11-01 09:37\\'}, \\'current\\': {\\'last_updated_epoch\\': 1730464200, \\'last_updated\\': \\'2024-11-01 09:30\\', \\'temp_c\\': 22.1, \\'temp_f\\': 71.8, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Sunny\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/113.png\\', \\'code\\': 1000}, \\'wind_mph\\': 2.2, \\'wind_kph\\': 3.6, \\'wind_degree\\': 52, \\'wind_dir\\': \\'NE\\', \\'pressure_mb\\': 1019.0, \\'pressure_in\\': 30.09, \\'precip_mm\\': 1.34, \\'precip_in\\': 0.05, \\'humidity\\': 73, \\'cloud\\': 0, \\'feelslike_c\\': 24.6, \\'feelslike_f\\': 76.2, \\'windchill_c\\': 20.6, \\'windchill_f\\': 69.1, \\'heatindex_c\\': 20.6, \\'heatindex_f\\': 69.1, \\'dewpoint_c\\': 16.8, \\'dewpoint_f\\': 62.3, \\'vis_km\\': 10.0, \\'vis_miles\\': 6.0, \\'uv\\': 6.0, \\'gust_mph\\': 6.7, \\'gust_kph\\': 10.8}}\"}, {\\'url\\': \\'https://reliefweb.int/report/brazil/brazil-severe-weather-inmet-media-echo-daily-flash-11-january-2024\\', \\'content\\': \\'Brazil. Brazil - Severe weather (INMET, media) (ECHO Daily Flash of 11 January 2024) Format News and Press Release Source. ECHO; Posted 11 Jan 2024 Originally published 11 Jan 2024\\'}]', name='tavily_search_results_json', tool_call_id='call_fmuYhr86x3iCLReoPRJzSmEc')]}\n",
      "{'messages': [AIMessage(content='The current weather in Bras√≠lia, Brazil, is 22.1¬∞C (71.8¬∞F) and sunny. Comparatively, San Francisco is cooler at 13.4¬∞C (56.0¬∞F) with patchy rain, and Madrid is 17.3¬∞C (63.1¬∞F) and partly cloudy. Bras√≠lia is warmer and sunnier than both San Francisco and Madrid.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 3335, 'total_tokens': 3414, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 2688}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'stop', 'logprobs': None}, id='run-868d5a1a-d377-4fbc-aeba-370b5477acda-0', usage_metadata={'input_tokens': 3335, 'output_tokens': 79, 'total_tokens': 3414, 'input_token_details': {'cache_read': 2688}, 'output_token_details': {'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Compare those weathers with the weather in Brazil\")]\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "#if we used thread_id=2, it wouldn't work cuz it wouldn't have context from the past\n",
    "#the memmory saver would be inside another separate thread\n",
    "\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We're using here a ``asynchronous`` memmory saver, which means that untill each llm/tool call is not done, we won't get any output\n",
    "\n",
    "In that way, we'll `stream tokens` too\n",
    "\n",
    "Basically, you'll see each token getting generated by the llm, one step at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from contextlib import AsyncExitStack\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "\n",
    "#this will allow us to stream tokens itselves too\n",
    "\n",
    "#we'll use A-Stream events\n",
    "#async method)\n",
    "\n",
    "\n",
    "#solution from error https://community.deeplearning.ai/t/lesson-4-persistence-and-streaming-attributeerror-generatorcontextmanager-object-has-no-attribute-get-next-version/697391/4\n",
    "stack = AsyncExitStack()\n",
    "memory = await stack.enter_async_context(AsyncSqliteSaver.from_conn_string(\":memory:\"))\n",
    "\n",
    "#----\n",
    "# from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "\n",
    "# memory = AsyncSqliteSaver.from_conn_string(\":memory:\")\n",
    "# abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'current weather in San Francisco'}, 'id': 'call_r7EBd8PUWuhiWqU5XuaA1plg', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "The| current| weather| in| San| Francisco| is| as| follows|:\n",
      "\n",
      "|-| Temperature|:| |13|.|4|¬∞C| (|56|.|0|¬∞F|)\n",
      "|-| Condition|:| Patch|y| rain| nearby|\n",
      "|-| Wind|:| |9|.|8| mph| (|15|.|8| k|ph|)| from| the| W|NW|\n",
      "|-| Hum|idity|:| |82|%\n",
      "|-| Cloud| Cover|:| |52|%\n",
      "|-| Visibility|:| |10| km| (|6| miles|)\n",
      "|-| UV| Index|:| |1| (|low|)\n",
      "\n",
      "|The| feels|-like| temperature| is| approximately| |12|.|1|¬∞C| (|53|.|7|¬∞F|).|"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in SF?\")]\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "async for event in abot.graph.astream_events({\"messages\": messages}, thread, version=\"v1\"):\n",
    "    kind = event[\"event\"]\n",
    "    \n",
    "    \n",
    "    #we retrieve the events when the llm is otuputting text\n",
    "    #and then we start showing token by token async\n",
    "    \n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        content = event[\"data\"][\"chunk\"].content\n",
    "        if content:\n",
    "            # Empty content in the context of OpenAI means\n",
    "            # that the model is asking for a tool to be invoked.\n",
    "            # So we only print non-empty content\n",
    "            print(content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'current price of tomatoes in Spain'}, 'id': 'call_UMDpECLoUaG6hQuJ8TwOK3M6', 'type': 'tool_call'}\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'current price of tomatoes in San Francisco'}, 'id': 'call_0I1IzBhpNcdRxP5pPSJ4mqSU', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "In| San| Francisco|,| the| average| price| for| |1| kilogram| (|approximately| |2| pounds|)| of| tomatoes| is| $|7|.\n",
      "\n",
      "|In| Spain|,| specifically| in| Barcelona|,| the| price| for| |1| kilogram| of| tomatoes| is| approximately| ‚Ç¨|2|.|00|.| Con|verting| this| to| US| dollars|,| given| the| current| exchange| rate|,| it| would| be| roughly| $|2|.|20| to| $|2|.|40| depending| on| the| exact| rate|.\n",
      "\n",
      "|Compar|atively|,| tomatoes| are| significantly| more| expensive| in| San| Francisco| than| in| Spain|.|"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"How much does a tomatoe cost at sf?. Compare it with the cost of it at spain\")]\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "async for event in abot.graph.astream_events({\"messages\": messages}, thread, version=\"v1\"):\n",
    "    kind = event[\"event\"]\n",
    "    \n",
    "    \n",
    "    #we retrieve the events when the llm is otuputting text\n",
    "    #and then we start showing token by token async\n",
    "    \n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        content = event[\"data\"][\"chunk\"].content\n",
    "        if content:\n",
    "            # Empty content in the context of OpenAI means\n",
    "            # that the model is asking for a tool to be invoked.\n",
    "            # So we only print non-empty content\n",
    "            print(content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exit all contexts\n",
    "# stack.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
